{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7295813-387f-4ce4-9d10-48f9a0c80e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading config and creating model skeleton for Qwen/Qwen1.5-MoE-A2.7B...\n",
      "2. Inferring device map (8GB VRAM limit enforced)...\n",
      "3. Downloading checkpoint and dispatching weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b6b52170694021b847d01a96c09770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37313469e7d49a2877d1fa561ad031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017d333ff5764717a781ffe757f4b14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3fd77462a7475fa23b988074fb6d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de154f6c5f3640aaa46aae5448c50326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6892165b16414858a6c35e193e0dd812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54c3056ee6347d4bbeba069e53ae1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbb90e2298c4aed9005838ffef8fb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model successfully loaded and dispatched across GPU and CPU RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Applying dynamic offloading patch to all MoE experts...\n",
      "\n",
      "==================================================\n",
      "6. Running generation for prompt: \n",
      "'Explain the concept of dynamic offloading in large language models in one paragraph.'\n",
      "\n",
      "--- GENERATION RESULT ---\n",
      "Explain the concept of dynamic offloading in large language models in one paragraph. Dynamic offloading is a technique used in large language models to improve their performance by offloading some of the computational tasks onto specialized hardware, such as graphics processing units\n",
      "==================================================\n",
      "Time taken in seconds: 82.98450660705566\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# ðŸ›‘ CRITICAL FIX 1: Set PyTorch CUDA memory configuration\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\" \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download \n",
    "import copy # <-- ADDED IMPORT\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Configuration\n",
    "# ------------------------------------------------------------\n",
    "model_id = \"Qwen/Qwen1.5-MoE-A2.7B\"       \n",
    "gpu_device = \"cuda:0\"                     \n",
    "cpu_device = \"cpu\"                         \n",
    "dtype = torch.float16                      \n",
    "\n",
    "# Use a conservative VRAM budget\n",
    "max_memory = {\n",
    "    0: \"8GiB\",        # Only 8 GB VRAM for the essential trunk/router\n",
    "    \"cpu\": \"300GiB\"   # Ample CPU memory for offloaded weights\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1 & 2 (No changes)\n",
    "# ------------------------------------------------------------\n",
    "print(f\"1. Loading config and creating model skeleton for {model_id}...\")\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "print(\"2. Inferring device map (8GB VRAM limit enforced)...\")\n",
    "no_split = getattr(empty_model, \"_no_split_modules\", [\"QwenBlock\", \"Block\"])\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    empty_model,\n",
    "    max_memory=max_memory, \n",
    "    no_split_module_classes=no_split,\n",
    ")\n",
    "\n",
    "for name in list(device_map.keys()):\n",
    "    if \"experts\" in name:\n",
    "        device_map[name] = cpu_device\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3 (No changes)\n",
    "# ------------------------------------------------------------\n",
    "print(\"3. Downloading checkpoint and dispatching weights...\")\n",
    "local_checkpoint_folder = snapshot_download(\n",
    "    model_id,\n",
    "    allow_patterns=[\"*.safetensors\", \"*.bin\", \"config.json\", \"*.json\"]\n",
    ")\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    empty_model,\n",
    "    checkpoint=local_checkpoint_folder, \n",
    "    device_map=device_map,\n",
    "    no_split_module_classes=no_split,\n",
    "    dtype=dtype,\n",
    "    offload_folder=None,\n",
    ")\n",
    "print(\"   Model successfully loaded and dispatched across GPU and CPU RAM.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Patch experts for dynamic CUDA computation (FIXED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"4. Applying dynamic offloading patch to all MoE experts...\")\n",
    "def patch_expert_for_cuda_compute(expert: nn.Module, gpu_device=\"cuda:0\"):\n",
    "    # Store a reference to the original, non-patched forward method for the CUDA clone\n",
    "    # This assumes the expert module is a standard nn.Module with a standard forward\n",
    "    # and has not been hooked (since it was just loaded onto CPU RAM).\n",
    "    \n",
    "    # We must retrieve the UNBOUND method here to manually re-create the module class\n",
    "    # The actual implementation of the expert is Qwen2MoeMLP in Qwen1.5-MoE\n",
    "    original_expert_class = expert.__class__ \n",
    "    cuda_dev = torch.device(gpu_device)\n",
    "\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        # 1. Move inputs to CUDA\n",
    "        args = [a.to(cuda_dev) if torch.is_tensor(a) else a for a in args]\n",
    "        kwargs = {k: (v.to(cuda_dev) if torch.is_tensor(v) else v) for k, v in kwargs.items()}\n",
    "\n",
    "        # 2. CRITICAL FIX 3: Manually instantiate a new module on CUDA and load weights\n",
    "        # Avoids the deep recursion issue caused by expert.to_empty() and accelerate hooks.\n",
    "        \n",
    "        # Get the config from the parent module if available, or just instantiate.\n",
    "        # Assuming the expert constructor takes no arguments or is wrapped by config\n",
    "        # This is a safe way to clone the structure.\n",
    "        \n",
    "        # NOTE: The expert module (Qwen2MoeMLP) is a simple structure, so this works:\n",
    "        expert_gpu = original_expert_class(config=expert.config).to(cuda_dev).to(dtype)\n",
    "        \n",
    "        # Load weights from CPU (DRAM) into the CUDA clone\n",
    "        expert_gpu.load_state_dict(expert.state_dict(), strict=True)\n",
    "        \n",
    "        # 3. Run forward on the temporary CUDA module\n",
    "        with torch.no_grad():\n",
    "            out = expert_gpu(*args, **kwargs)\n",
    "\n",
    "        # 4. Free the CUDA clone to release VRAM immediately\n",
    "        del expert_gpu\n",
    "        torch.cuda.empty_cache()\n",
    "        return out\n",
    "\n",
    "    # We replace the forward of the CPU-resident module with our wrapper\n",
    "    expert.forward = wrapped_forward\n",
    "\n",
    "# Apply the patch to every expert ModuleList\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.ModuleList) and \"experts\" in name:\n",
    "        for expert in module:\n",
    "            # Check if the expert is actually on the CPU before patching\n",
    "            # The device map might have put small experts on CUDA.\n",
    "            if next(expert.parameters()).device.type == cpu_device:\n",
    "                patch_expert_for_cuda_compute(expert, gpu_device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 5 & 6 (No changes)\n",
    "# ------------------------------------------------------------\n",
    "model.config.use_cache = False\n",
    "model.eval()\n",
    "try:\n",
    "    model.config.attn_implementation = \"flash_attention_2\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "PROMPT = \"Explain the concept of dynamic offloading in large language models in one paragraph.\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"6. Running generation for prompt: \\n'{PROMPT}'\")\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(gpu_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc9816e-85b7-4950-bbd2-be86e38a9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GENERATION RESULT ---\n",
      "Explain the concept of dynamic offloading in large language models in one paragraph. Dynamic offloading in large language models refers to a technique where the computation is split between different devices, such as GPUs or TPUs, to reduce the amount of memory and computational resources required by the model. This technique allows for faster training and inference times, as well as improved accuracy, by allowing the model to train on\n",
      "==================================================\n",
      "Time taken in seconds: 185.41313791275024\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t2 = time.time()\n",
    "start = time.time()\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "print(\"\\n--- GENERATION RESULT ---\")\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "print(\"=\"*50)\n",
    "t1 = time.time()\n",
    "print(\"Time taken in seconds:\",t1-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1d92f9-7c63-4536-ae35-9a5c624d2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a386769-87e7-477e-a59d-e29607ca1596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading config and creating model skeleton for Qwen/Qwen1.5-MoE-A2.7B...\n",
      "2. Inferring device map (8GB VRAM limit enforced)...\n",
      "3. Downloading checkpoint and dispatching weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7760ad813d4e34a09ddf88fb317e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66092d6717cd41c18d57b682538de317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd47306ee604917918f49e1d225fd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b89af1e1c145498fa9b90d270c585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499dd0e07cfc483e8e7f673f8c2bd8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e65cdfb40d4967882bbe817cf2a635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8cdca4bfb7442aac2cf30dde6f2636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model successfully loaded and dispatched across GPU and CPU RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 0 CPU-resident experts for dynamic caching.\n",
      "Patched 0 experts with persistent CUDA-cache wrapper.\n",
      "\n",
      "========================================================================\n",
      "Running generation for prompt:\n",
      "Explain the concept of dynamic offloading in large language models in one paragraph.\n",
      "\n",
      "\n",
      "--- GENERATION RESULT ---\n",
      "Explain the concept of dynamic offloading in large language models in one paragraph. Additionally, provide a code snippet that demonstrates the implementation of dynamic offloading in a Python library for large language models.\n",
      "\n",
      "Dynamic offloading is a technique used to improve the performance of large language models by dynamically offloading portions of the computation to specialized hardware, such as GPUs or TPUs. It allows the model to benefit from\n",
      "\n",
      "Time: 188.66s\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 0) CUDA allocator safety: expandable segments reduce fragmentation on churny alloc/free.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "model_id   = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "gpu_device = \"cuda:0\"\n",
    "cpu_device = \"cpu\"\n",
    "dtype      = torch.float16\n",
    "\n",
    "# VRAM budget for trunk/router; experts will be cached separately via our LRU\n",
    "max_memory = {\n",
    "    0:   \"8GiB\",\n",
    "    \"cpu\":\"300GiB\"\n",
    "}\n",
    "\n",
    "# Expert cache caps (choose one primary; the other is a guardrail)\n",
    "GPU_CACHE_MAX_BYTES   = 2_000_000_000   # ~2.0 GB for expert weights total\n",
    "GPU_CACHE_MAX_EXPERTS = 8               # or at most 8 experts resident on GPU\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load config & empty skeleton\n",
    "# -----------------------------\n",
    "print(f\"1. Loading config and creating model skeleton for {model_id}...\")\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Build device map (experts to CPU)\n",
    "# -----------------------------\n",
    "print(\"2. Inferring device map (8GB VRAM limit enforced)...\")\n",
    "no_split = getattr(empty_model, \"_no_split_modules\", [\"QwenBlock\", \"Block\"])\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    empty_model,\n",
    "    max_memory=max_memory,\n",
    "    no_split_module_classes=no_split,\n",
    ")\n",
    "for name in list(device_map.keys()):\n",
    "    if \"experts\" in name:\n",
    "        device_map[name] = cpu_device\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Download checkpoints & dispatch\n",
    "# -----------------------------\n",
    "print(\"3. Downloading checkpoint and dispatching weights...\")\n",
    "local_ckpt = snapshot_download(\n",
    "    model_id,\n",
    "    allow_patterns=[\"*.safetensors\", \"*.bin\", \"config.json\", \"*.json\"]\n",
    ")\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    empty_model,\n",
    "    checkpoint=local_ckpt,\n",
    "    device_map=device_map,\n",
    "    no_split_module_classes=no_split,\n",
    "    dtype=dtype,\n",
    "    offload_folder=None,\n",
    ")\n",
    "print(\"   Model successfully loaded and dispatched across GPU and CPU RAM.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ======================================================================\n",
    "#                   EXPERT GPU LRU CACHE (CORE UPGRADE)\n",
    "# ======================================================================\n",
    "\n",
    "def tensor_nbytes(t: torch.Tensor) -> int:\n",
    "    # Handles fp16/bf16 int8 â€¦; assumes dense tensors\n",
    "    return t.numel() * t.element_size()\n",
    "\n",
    "def module_param_bytes(mod: nn.Module) -> int:\n",
    "    s = 0\n",
    "    for p in mod.parameters(recurse=True):\n",
    "        s += tensor_nbytes(p.data)\n",
    "    for b in mod.buffers(recurse=True):\n",
    "        s += tensor_nbytes(b.data)\n",
    "    return s\n",
    "\n",
    "def pin_state_dict(cpu_module: nn.Module):\n",
    "    \"\"\"\n",
    "    Create a pinned-memory state_dict snapshot for faster Hâ†’D copies.\n",
    "    (One-time cost per expert.)\n",
    "    \"\"\"\n",
    "    pinned = {}\n",
    "    with torch.no_grad():\n",
    "        for k, v in cpu_module.state_dict().items():\n",
    "            # ensure CPU & contiguous for consistent pinning\n",
    "            t = v.detach().contiguous().to(\"cpu\", copy=True)\n",
    "            # pin if pageable (CUDA only supports pinning CPU tensors)\n",
    "            try:\n",
    "                t = t.pin_memory()\n",
    "            except RuntimeError:\n",
    "                # Some dtypes (e.g., bf16 on older builds) may not support pinning; fall back\n",
    "                pass\n",
    "            pinned[k] = t\n",
    "    return pinned\n",
    "\n",
    "class ExpertGPUCache:\n",
    "    \"\"\"\n",
    "    LRU cache for CUDA-resident expert modules.\n",
    "    - Capacity tracked by both bytes and count.\n",
    "    - Uses pinned CPU state_dict for fast, async, non_blocking loads.\n",
    "    - Optional prefetch using a dedicated CUDA stream.\n",
    "    \"\"\"\n",
    "    def __init__(self, device:str, dtype:torch.dtype,\n",
    "                 max_bytes:int = GPU_CACHE_MAX_BYTES,\n",
    "                 max_count:int = GPU_CACHE_MAX_EXPERTS):\n",
    "        self.device = torch.device(device)\n",
    "        self.dtype  = dtype\n",
    "        self.max_bytes = max_bytes\n",
    "        self.max_count = max_count\n",
    "\n",
    "        self._cache: OrderedDict[int, nn.Module] = OrderedDict()  # expert_id -> cuda module\n",
    "        self._sizes: dict[int, int] = {}                          # bytes by expert\n",
    "        self._pinned_sd: dict[int, dict] = {}                     # expert_id -> pinned state_dict\n",
    "        self._bytes_total = 0\n",
    "\n",
    "        # Single prefetch stream for async Hâ†’D copies\n",
    "        self.prefetch_stream = torch.cuda.Stream(device=self.device)\n",
    "        self.main_stream = torch.cuda.current_stream(device=self.device)\n",
    "\n",
    "    def _ensure_capacity(self, need_bytes:int):\n",
    "        # Evict LRU until enough room (by bytes & count)\n",
    "        while (self._bytes_total + need_bytes > self.max_bytes) or \\\n",
    "              (len(self._cache) + 1 > self.max_count):\n",
    "            # Pop LRU (first item)\n",
    "            evict_id, evict_mod = self._cache.popitem(last=False)\n",
    "            ev_sz = self._sizes.pop(evict_id, 0)\n",
    "            self._bytes_total -= ev_sz\n",
    "            # Free module params (no empty_cache; allocator will reuse)\n",
    "            del evict_mod\n",
    "        # Synchronization not strictly required here; allocator reuse is fine.\n",
    "\n",
    "    def _build_cuda_module(self, cpu_expert: nn.Module) -> nn.Module:\n",
    "        # Instantiate a fresh CUDA expert (structure only)\n",
    "        # Prefer constructor with config; fallback to deepcopy if needed.\n",
    "        cls = cpu_expert.__class__\n",
    "        if hasattr(cpu_expert, \"config\"):\n",
    "            m = cls(config=cpu_expert.config).to(self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            # Fallback (rare): deepcopy structure, then move to CUDA & reset params\n",
    "            import copy as _copy\n",
    "            m = _copy.deepcopy(cpu_expert).to(self.device, dtype=self.dtype)\n",
    "            # state_dict will be overwritten anyway\n",
    "        return m\n",
    "\n",
    "    def _load_weights_async(self, cuda_mod: nn.Module, expert_id: int):\n",
    "        \"\"\"\n",
    "        Load from pinned CPU state_dict into CUDA module using non_blocking\n",
    "        on the prefetch stream; then make main stream wait for completion.\n",
    "        \"\"\"\n",
    "        if expert_id not in self._pinned_sd:\n",
    "            # One-time pinned snapshot\n",
    "            self._pinned_sd[expert_id] = pin_state_dict(cpu_expert_registry[expert_id])\n",
    "\n",
    "        sd = self._pinned_sd[expert_id]\n",
    "        # non_blocking Hâ†’D: do it key-by-key to avoid a large blocking call\n",
    "        with torch.cuda.stream(self.prefetch_stream):\n",
    "            for k, v_cpu in sd.items():\n",
    "                # torch.nn.Module.load_state_dict does not expose non_blocking per key,\n",
    "                # so we copy tensors into the parameter/buffer directly.\n",
    "                # Map k to actual tensor ref in cuda_mod:\n",
    "                ref = cuda_mod\n",
    "                comps = k.split(\".\")\n",
    "                for c in comps[:-1]:\n",
    "                    ref = getattr(ref, c)\n",
    "                leaf_name = comps[-1]\n",
    "                # Select parameter/buffer tensor\n",
    "                if hasattr(ref, leaf_name):\n",
    "                    t = getattr(ref, leaf_name)\n",
    "                    # Copy data into place\n",
    "                    t.data.copy_(v_cpu, non_blocking=True)\n",
    "                else:\n",
    "                    # In unusual cases (e.g., missing buffer), fall back to set_\n",
    "                    pass\n",
    "\n",
    "        # Make main stream wait for prefetch stream completion to ensure data is ready\n",
    "        self.main_stream.wait_stream(self.prefetch_stream)\n",
    "        torch.cuda.current_stream(self.device).synchronize()  # keep it simple & safe\n",
    "\n",
    "    def get(self, expert_id: int, cpu_expert: nn.Module) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Return a CUDA-resident expert ready for compute.\n",
    "        If not cached, instantiate & async-load; maintain LRU.\n",
    "        \"\"\"\n",
    "        # Fast path: cache hit â†’ move to MRU\n",
    "        if expert_id in self._cache:\n",
    "            mod = self._cache.pop(expert_id)\n",
    "            self._cache[expert_id] = mod  # MRU\n",
    "            return mod\n",
    "\n",
    "        # Miss â†’ build, ensure capacity, async-load, account size, insert as MRU\n",
    "        cuda_mod = self._build_cuda_module(cpu_expert)\n",
    "        need_bytes = module_param_bytes(cuda_mod)\n",
    "\n",
    "        self._ensure_capacity(need_bytes)\n",
    "        self._load_weights_async(cuda_mod, expert_id)\n",
    "\n",
    "        self._cache[expert_id] = cuda_mod  # MRU\n",
    "        self._sizes[expert_id] = need_bytes\n",
    "        self._bytes_total += need_bytes\n",
    "        return cuda_mod\n",
    "\n",
    "    # Optional: allow ahead-of-time prefetch (if you can obtain router top-k indices)\n",
    "    def prefetch_experts(self, ids: list[int]):\n",
    "        with torch.no_grad():\n",
    "            for eid in ids:\n",
    "                if eid in self._cache:\n",
    "                    # Touch to mark MRU\n",
    "                    mod = self._cache.pop(eid)\n",
    "                    self._cache[eid] = mod\n",
    "                else:\n",
    "                    if eid not in cpu_expert_registry:\n",
    "                        continue\n",
    "                    cpu_mod = cpu_expert_registry[eid]\n",
    "                    cuda_mod = self._build_cuda_module(cpu_mod)\n",
    "                    need_bytes = module_param_bytes(cuda_mod)\n",
    "                    self._ensure_capacity(need_bytes)\n",
    "                    self._load_weights_async(cuda_mod, eid)\n",
    "                    self._cache[eid] = cuda_mod\n",
    "                    self._sizes[eid] = need_bytes\n",
    "                    self._bytes_total += need_bytes\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Build a registry mapping expert_id -> CPU expert module for quick access\n",
    "# Qwen MoE usually has ModuleList named like \"...experts\", each item is one expert.\n",
    "# We'll assign a global counter id in discovery order.\n",
    "# ------------------------------------------------------------------------------\n",
    "cpu_expert_registry: dict[int, nn.Module] = {}\n",
    "expert_name_map: dict[int, str] = {}  # id -> dotted path (debug)\n",
    "eid = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.ModuleList) and \"experts\" in name:\n",
    "        for idx, expert in enumerate(module):\n",
    "            # only register experts that are on CPU\n",
    "            try:\n",
    "                dev = next(expert.parameters()).device.type\n",
    "            except StopIteration:\n",
    "                dev = cpu_device\n",
    "            if dev == cpu_device:\n",
    "                cpu_expert_registry[eid] = expert\n",
    "                expert_name_map[eid] = f\"{name}[{idx}]\"\n",
    "                eid += 1\n",
    "\n",
    "print(f\"Discovered {len(cpu_expert_registry)} CPU-resident experts for dynamic caching.\")\n",
    "\n",
    "# Create one global cache manager\n",
    "expert_cache = ExpertGPUCache(device=gpu_device, dtype=dtype,\n",
    "                              max_bytes=GPU_CACHE_MAX_BYTES, max_count=GPU_CACHE_MAX_EXPERTS)\n",
    "\n",
    "# ======================================================================\n",
    "#             PATCH EXPERT FORWARD TO USE THE LRU CACHE (NO EMPTY_CACHE)\n",
    "# ======================================================================\n",
    "def make_cached_forward(expert_id: int, cpu_expert: nn.Module):\n",
    "    \"\"\"\n",
    "    Wrap CPU expert.forward so that it:\n",
    "      - Moves inputs to GPU (non_blocking where possible)\n",
    "      - Gets/creates a cached CUDA expert from LRU\n",
    "      - Runs compute on GPU\n",
    "      - Returns result on original device/dtype (match upstream expectations)\n",
    "    \"\"\"\n",
    "    orig_fwd = cpu_expert.forward  # not used directly but kept for safety\n",
    "\n",
    "    def wrapped_forward(*args, **kwargs):\n",
    "        # Detect target device for inputs\n",
    "        cuda_dev = torch.device(gpu_device)\n",
    "\n",
    "        # Move tensor args/kwargs to GPU non_blocking, keep non-tensors as-is\n",
    "        def to_cuda(x):\n",
    "            if torch.is_tensor(x):\n",
    "                # If x is already on GPU, return as-is; else move non_blocking if possible.\n",
    "                return x.to(cuda_dev, non_blocking=True)\n",
    "            return x\n",
    "\n",
    "        args_cuda = tuple(to_cuda(a) for a in args)\n",
    "        kwargs_cuda = {k: to_cuda(v) for k, v in kwargs.items()}\n",
    "\n",
    "        # Get CUDA expert from LRU cache (instantiates+loads asynchronously on miss)\n",
    "        cuda_expert = expert_cache.get(expert_id, cpu_expert)\n",
    "\n",
    "        # Compute (no grad during inference). If you need grad, remove this context.\n",
    "        with torch.no_grad():\n",
    "            out = cuda_expert(*args_cuda, **kwargs_cuda)\n",
    "\n",
    "        return out\n",
    "\n",
    "    return wrapped_forward\n",
    "\n",
    "# Apply wrappers to each CPU expert\n",
    "patched = 0\n",
    "for eid, cpu_expert in cpu_expert_registry.items():\n",
    "    cpu_expert.forward = make_cached_forward(eid, cpu_expert)\n",
    "    patched += 1\n",
    "print(f\"Patched {patched} experts with persistent CUDA-cache wrapper.\")\n",
    "\n",
    "# ======================================================================\n",
    "#                     Model Gen Settings (unchanged)\n",
    "# ======================================================================\n",
    "model.config.use_cache = False  # keep disabled; experts are transiently-GPU\n",
    "model.eval()\n",
    "try:\n",
    "    model.config.attn_implementation = \"flash_attention_2\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ======================================================================\n",
    "#                              Demo\n",
    "# ======================================================================\n",
    "PROMPT = \"Explain the concept of dynamic offloading in large language models in one paragraph.\"\n",
    "print(\"\\n\" + \"=\"*72)\n",
    "print(f\"Running generation for prompt:\\n{PROMPT}\\n\")\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(gpu_device)\n",
    "\n",
    "# (Optional) If you can peek router top-k ids, call expert_cache.prefetch_experts(ids) here.\n",
    "\n",
    "start = time.time()\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.05,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=False\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "print(\"\\n--- GENERATION RESULT ---\")\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "print(f\"\\nTime: {end-start:.2f}s\")\n",
    "print(\"=\"*72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d96a0e6-c3f3-4423-bcda-1025461515a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Building model for A_auto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf945821b5a4657b993dffb38c02bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CASE A (auto map)] VRAM after load: alloc=20182.2 MB, reserved=20186.0 MB\n",
      "[CASE A (auto map)] Max feasible batch size = 12\n",
      "[CASE A (auto map)] 3072 tokens in 84.44s â†’ 36.38 tok/s\n",
      "[CASE A (auto map)] VRAM after run: alloc=20190.3 MB, reserved=23866.0 MB\n",
      "\n",
      "==> Building model for B_cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9388995cef9444a981362e1cbc51cf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ce0d10f53a4d7b965843b3e314df2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/578 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4904d15520304d509d4cee5ab5123e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8c1086ec074f8ba07dfc1e9fa8a684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/677 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679172f017d2429eb322a2024882d13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/681 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CASE B (experts on CPU)] VRAM after load: alloc=6042.9 MB, reserved=6082.0 MB\n",
      "[CASE B (experts on CPU)] Max feasible batch size = 70\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 598.94 MiB is free. Process 762405 has 23.07 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 108.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 146\u001b[0m\n\u001b[1;32m    143\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(GPU)\n\u001b[1;32m    145\u001b[0m resA \u001b[38;5;241m=\u001b[39m benchmark_case(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCASE A (auto map)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m resB \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_case\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCASE B (experts on CPU)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB_cpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m================ SUMMARY ================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCase A (auto):  max_batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_batch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, TPS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, VRAM@load=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresA[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvram\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m, in \u001b[0;36mbenchmark_case\u001b[0;34m(case_name, case_flag)\u001b[0m\n\u001b[1;32m    128\u001b[0m max_bs \u001b[38;5;241m=\u001b[39m find_max_batch(model, tok, GPU, SEARCH_START_BS, SEARCH_MAX_BS_HINT)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Max feasible batch size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_bs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m secs \u001b[38;5;241m=\u001b[39m \u001b[43mtry_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGPU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m max_bs \u001b[38;5;241m*\u001b[39m MAX_NEW_TOKENS\n\u001b[1;32m    133\u001b[0m tps \u001b[38;5;241m=\u001b[39m total_tokens \u001b[38;5;241m/\u001b[39m secs\n",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m, in \u001b[0;36mtry_one\u001b[0;34m(model, tokenizer, batch_size, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 85\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     93\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2530\u001b[0m         inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2535\u001b[0m     )\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2552\u001b[0m         input_ids,\n\u001b[1;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2558\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2867\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2864\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2867\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2868\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:940\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 940\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    942\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:1117\u001b[0m, in \u001b[0;36mQwen2MoeForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1113\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1114\u001b[0m )\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m outputs: MoeModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:940\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 940\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    942\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:862\u001b[0m, in \u001b[0;36mQwen2MoeModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, cache_position)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    860\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 862\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:730\u001b[0m, in \u001b[0;36mQwen2MoeDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, output_attentions, output_router_logits, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 730\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden_states, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    732\u001b[0m     hidden_states, router_logits \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:644\u001b[0m, in \u001b[0;36mQwen2MoeSparseMoeBlock.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;66;03m# However `index_add_` only support torch tensors for indexing so we'll use\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# the `top_x` tensor here.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     final_hidden_states\u001b[38;5;241m.\u001b[39mindex_add_(\u001b[38;5;241m0\u001b[39m, top_x, current_hidden_states\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m--> 644\u001b[0m shared_expert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_expert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m shared_expert_output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_expert_gate(hidden_states)) \u001b[38;5;241m*\u001b[39m shared_expert_output\n\u001b[1;32m    647\u001b[0m final_hidden_states \u001b[38;5;241m=\u001b[39m final_hidden_states \u001b[38;5;241m+\u001b[39m shared_expert_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:175\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py:256\u001b[0m, in \u001b[0;36mQwen2MoeMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 770.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 598.94 MiB is free. Process 762405 has 23.07 GiB memory in use. Of the allocated memory 22.65 GiB is allocated by PyTorch, and 108.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os, gc, time\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "MODEL_ID   = \"Qwen/Qwen1.5-MoE-A2.7B\"\n",
    "DTYPE      = torch.float16\n",
    "GPU        = \"cuda:0\"\n",
    "CPU        = \"cpu\"\n",
    "\n",
    "# ---------------- Benchmark knobs ----------------\n",
    "TARGET_CTX_TOKENS   = 1024     # input context length\n",
    "MAX_NEW_TOKENS      = 256      # number of tokens to decode\n",
    "SEARCH_START_BS     = 1        # batch size lower bound\n",
    "SEARCH_MAX_BS_HINT  = 8        # initial upper bound for batch search\n",
    "\n",
    "MAX_MEMORY = {0: \"8GiB\", \"cpu\": \"300GiB\"}\n",
    "\n",
    "def vram_mb():\n",
    "    return (torch.cuda.memory_allocated() / 1024**2,\n",
    "            torch.cuda.memory_reserved()  / 1024**2)\n",
    "\n",
    "def make_inputs(tokenizer, batch_size:int, device:str, target_ctx:int):\n",
    "    seed_sentence = \"In large language models, the KV cache must remain on the GPU for low-latency decoding. \"\n",
    "    toks = tokenizer(seed_sentence, return_tensors=\"pt\").input_ids[0]\n",
    "    reps = max(1, (target_ctx // toks.numel()) + 1)\n",
    "    long_ids = toks.repeat(reps)[:target_ctx].unsqueeze(0)\n",
    "    input_ids = long_ids.repeat(batch_size, 1)\n",
    "    attn = torch.ones_like(input_ids)\n",
    "    return {\"input_ids\": input_ids.to(device), \"attention_mask\": attn.to(device)}\n",
    "\n",
    "def build_model(case:str):\n",
    "    print(f\"\\n==> Building model for {case}\")\n",
    "    if case == \"A_auto\":\n",
    "        # Use accelerate's auto map\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=DTYPE\n",
    "        )\n",
    "    else:  # Case B, experts on CPU\n",
    "        config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "        with init_empty_weights():\n",
    "            empty = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "        no_split = getattr(empty, \"_no_split_modules\", [\"QwenBlock\", \"Block\"])\n",
    "        device_map = infer_auto_device_map(\n",
    "            empty,\n",
    "            max_memory=MAX_MEMORY,\n",
    "            no_split_module_classes=no_split\n",
    "        )\n",
    "        for name in list(device_map.keys()):\n",
    "            if \"experts\" in name:\n",
    "                device_map[name] = CPU\n",
    "\n",
    "        local_ckpt = snapshot_download(\n",
    "            MODEL_ID, allow_patterns=[\"*.safetensors\",\"*.bin\",\"config.json\",\"*.json\"]\n",
    "        )\n",
    "        model = load_checkpoint_and_dispatch(\n",
    "            empty,\n",
    "            checkpoint=local_ckpt,\n",
    "            device_map=device_map,\n",
    "            no_split_module_classes=no_split,\n",
    "            dtype=DTYPE,\n",
    "            offload_folder=None,\n",
    "        )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    model.config.use_cache = True\n",
    "    try: model.config.attn_implementation = \"flash_attention_2\"\n",
    "    except Exception: pass\n",
    "    model.eval()\n",
    "    return model, tok\n",
    "\n",
    "def try_one(model, tokenizer, batch_size:int, device:str):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    inputs = make_inputs(tokenizer, batch_size, device, TARGET_CTX_TOKENS)\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def find_max_batch(model, tokenizer, device:str, start_bs:int, max_hint:int):\n",
    "    def feasible(bs:int):\n",
    "        try:\n",
    "            _ = try_one(model, tokenizer, bs, device)\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                return False\n",
    "            raise\n",
    "\n",
    "    lo, hi = start_bs, max_hint\n",
    "    best = lo\n",
    "    while feasible(hi):\n",
    "        best = hi\n",
    "        hi *= 2\n",
    "        if hi > 128: break\n",
    "\n",
    "    L, R = best+1, hi\n",
    "    while L <= R:\n",
    "        mid = (L + R)//2\n",
    "        if feasible(mid):\n",
    "            best = mid\n",
    "            L = mid+1\n",
    "        else:\n",
    "            R = mid-1\n",
    "    return best\n",
    "\n",
    "def benchmark_case(case_name, case_flag):\n",
    "    model, tok = build_model(case_flag)\n",
    "    alloc, reserv = vram_mb()\n",
    "    print(f\"[{case_name}] VRAM after load: alloc={alloc:.1f} MB, reserved={reserv:.1f} MB\")\n",
    "\n",
    "    max_bs = find_max_batch(model, tok, GPU, SEARCH_START_BS, SEARCH_MAX_BS_HINT)\n",
    "    print(f\"[{case_name}] Max feasible batch size = {max_bs}\")\n",
    "\n",
    "    secs = try_one(model, tok, max_bs, GPU)\n",
    "    total_tokens = max_bs * MAX_NEW_TOKENS\n",
    "    tps = total_tokens / secs\n",
    "    print(f\"[{case_name}] {total_tokens} tokens in {secs:.2f}s â†’ {tps:.2f} tok/s\")\n",
    "\n",
    "    alloc2, reserv2 = vram_mb()\n",
    "    print(f\"[{case_name}] VRAM after run: alloc={alloc2:.1f} MB, reserved={reserv2:.1f} MB\")\n",
    "\n",
    "    del model; del tok; gc.collect(); torch.cuda.empty_cache()\n",
    "    return {\"max_batch\": max_bs, \"tps\": tps, \"vram\": alloc}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.set_device(GPU)\n",
    "\n",
    "    resA = benchmark_case(\"CASE A (auto map)\", \"A_auto\")\n",
    "    resB = benchmark_case(\"CASE B (experts on CPU)\", \"B_cpu\")\n",
    "\n",
    "    print(\"\\n================ SUMMARY ================\")\n",
    "    print(f\"Case A (auto):  max_batch={resA['max_batch']}, TPS={resA['tps']:.2f}, VRAM@load={resA['vram']:.1f} MB\")\n",
    "    print(f\"Case B (CPU):   max_batch={resB['max_batch']}, TPS={resB['tps']:.2f}, VRAM@load={resB['vram']:.1f} MB\")\n",
    "    if resA['max_batch'] > 0:\n",
    "        print(f\"Batch capacity gain = {resB['max_batch']/resA['max_batch']:.2f}Ã—\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fe9fe4-6653-4a65-9d33-ed4fac684cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 1: `sudo kill -9 <pid>'\n",
      "Disabled persistence mode for GPU 00000000:43:00.0.\n",
      "All done.\n",
      "GPU Reset couldn't run because GPU 00000000:43:00.0 is the primary GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Kill any leftover processes holding VRAM (replace <pid>):\n",
    "!sudo kill -9 <pid>\n",
    "\n",
    "# Optional: reset/persistence off (if allowed; may require root and no active procs)\n",
    "!sudo nvidia-smi -pm 0\n",
    "!sudo nvidia-smi --gpu-reset -i 0   # CAREFUL: only if no important jobs running\n",
    "\n",
    "# If you run from Jupyter/Colab/notebook, restart the kernel/process too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c408724-95c6-4cf6-affd-b96bb685a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: use GPU 0 only (change to the one with most free memory)\n",
    "!export CUDA_VISIBLE_DEVICES=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b29f4b-1342-47af-8220-3587cbbed0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lmcache\n",
      "  Downloading lmcache-0.3.6-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cufile-python\n",
      "  Downloading cufile_python-0.1.1-py3-none-any.whl (4.9 kB)\n",
      "Collecting nvtx\n",
      "  Downloading nvtx-0.2.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (474 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.2/474.2 KB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setuptools_scm>=8\n",
      "  Downloading setuptools_scm-9.2.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 KB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from lmcache) (3.12.15)\n",
      "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from lmcache) (0.19.0)\n",
      "Collecting awscrt\n",
      "  Downloading awscrt-0.28.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from lmcache) (0.6.2)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /usr/local/lib/python3.10/dist-packages (from lmcache) (27.1.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from lmcache) (0.22.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lmcache) (2.2.6)\n",
      "Requirement already satisfied: transformers>=4.51.1 in /usr/local/lib/python3.10/dist-packages (from lmcache) (4.56.2)\n",
      "Collecting aiofile\n",
      "  Downloading aiofile-3.9.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from lmcache) (7.0.0)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lmcache) (2.8.0)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Collecting redis\n",
      "  Downloading redis-6.4.0-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m279.8/279.8 KB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools<81.0.0,>=77.0.3 in /usr/local/lib/python3.10/dist-packages (from lmcache) (80.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lmcache) (6.0.2)\n",
      "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from setuptools_scm>=8->lmcache) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from setuptools_scm>=8->lmcache) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (0.35.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (0.22.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (2025.9.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.51.1->lmcache) (4.67.1)\n",
      "Collecting caio<0.10.0,>=0.9.0\n",
      "  Downloading caio-0.9.24-cp310-cp310-manylinux_2_34_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.7/77.7 KB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (1.20.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (6.6.4)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lmcache) (1.4.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.5.8.93)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.90)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (4.15.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (11.3.3.83)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (1.14.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.90)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (3.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (1.13.1.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->lmcache) (3.4.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.51.1->lmcache) (1.1.10)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->lmcache) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->lmcache) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lmcache) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.51.1->lmcache) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.51.1->lmcache) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.51.1->lmcache) (2025.8.3)\n",
      "Installing collected packages: sortedcontainers, nvtx, setuptools_scm, redis, cufile-python, caio, awscrt, aiofiles, aiofile, lmcache\n",
      "Successfully installed aiofile-3.9.0 aiofiles-24.1.0 awscrt-0.28.1 caio-0.9.24 cufile-python-0.1.1 lmcache-0.3.6 nvtx-0.2.13 redis-6.4.0 setuptools_scm-9.2.0 sortedcontainers-2.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m[GPU] Using GPU 0 (free 23.67 / total 23.99 GiB)\n",
      "[GPU] gpu_memory_utilization set to 0.700 (conservative limit)\n",
      "[CPU/CXL] Configuring LMCache for 16 GB offload size...\n",
      "Building engine for Qwen/Qwen1.5-MoE-A2.7B (CPU/CXL offload + Prefix Caching)â€¦\n",
      "INFO 09-28 16:01:14 [utils.py:328] non-default args: {'dtype': 'half', 'max_model_len': 8192, 'enable_prefix_caching': True, 'cpu_offload_gb': 16, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'enforce_eager': True, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='b97971e1-1552-4813-96ec-37228d14baad', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None), 'model': 'Qwen/Qwen1.5-MoE-A2.7B'}\n",
      "INFO 09-28 16:01:15 [__init__.py:742] Resolved architecture: Qwen2MoeForCausalLM\n",
      "WARNING 09-28 16:01:15 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-28 16:01:15 [__init__.py:1815] Using max model len 8192\n",
      "INFO 09-28 16:01:16 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-28 16:01:16 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:17 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:17 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen1.5-MoE-A2.7B', speculative_config=None, tokenizer='Qwen/Qwen1.5-MoE-A2.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen1.5-MoE-A2.7B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:19 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:19 [factory.py:51] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: b97971e1-1552-4813-96ec-37228d14baad\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m WARNING 09-28 16:01:19 [base.py:84] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:01:19,232] LMCache WARNING:\u001b[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. \u001b[3m(utils.py:48:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:01:19,233] LMCache WARNING:\u001b[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE \u001b[3m(utils.py:52:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:19,234] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '5.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:19,235] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '16.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:19,236] LMCache INFO:\u001b[0m use mla: False, kv shape: (24, 2, 256, 16, 128), num_mtp_layers:0 \u001b[3m(vllm_v1_adapter.py:460:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:19,241] LMCache INFO:\u001b[0m Creating LMCacheEngine instance vllm-instance \u001b[3m(cache_engine.py:1447:lmcache.v1.cache_engine)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:19,242] LMCache INFO:\u001b[0m NUMA mapping for instance vllm-instance: None \u001b[3m(cache_engine.py:1450:lmcache.v1.cache_engine)\u001b[0m\n",
      "[W928 16:01:19.243755626 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:27,256] LMCache INFO:\u001b[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 16.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(cache_engine.py:95:lmcache.v1.cache_engine)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:27,258] LMCache INFO:\u001b[0m Initializing LRUCachePolicy \u001b[3m(lru.py:20:lmcache.v1.storage_backend.cache_policy.lru)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:27,259] LMCache INFO:\u001b[0m Initializing usage context. \u001b[3m(usage_context.py:261:lmcache.usage_context)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:32,448] LMCache INFO:\u001b[0m lmcache lookup server start on ipc:///tmp/engine_b97971e1-1552-4813-96ec-37228d14baad_service_lookup_lmcache_rpc_port_0 \u001b[3m(lmcache_lookup_client.py:215:lmcache.v1.lookup_client.lmcache_lookup_client)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:32,450] LMCache INFO:\u001b[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None \u001b[3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:01:32,451] LMCache INFO:\u001b[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.6-gcad4db105, vllm version 0.10.2, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen1.5-MoE-A2.7B', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.float16, kv_shape=(24, 2, 256, 16, 128), use_mla=False) \u001b[3m(vllm_v1_adapter.py:668:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m WARNING 09-28 16:01:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:32 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen1.5-MoE-A2.7B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:32 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:32 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:50 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4709978fe44f249c0651ee177d568a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:56 [default_loader.py:268] Loading weights took 5.03 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:01:56 [gpu_model_runner.py:2392] Model loading took 10.0502 GiB and 23.336833 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m WARNING 09-28 16:01:57 [fused_moe.py:727] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=60,N=1408,device_name=NVIDIA_GeForce_RTX_3090_Ti.json']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:18 [gpu_worker.py:298] Available KV cache memory: 4.98 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:19 [kv_cache_utils.py:864] GPU KV cache size: 27,200 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:19 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 3.32x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:19 [utils.py:114] Connectors do not specify a kv cache layout, defaulting to NHD.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:19 [gpu_worker.py:391] Free memory on device (23.36/23.66 GiB) on startup. Desired GPU memory utilization is (0.7, 16.57 GiB). Actual usage is 10.05 GiB for weight, 1.42 GiB for peak activation, 0.12 GiB for non-torch memory, and 0.0 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=5192511590` to fit into requested memory, or `--kv-cache-memory=12490476032` to fully utilize gpu memory. Current kv cache memory in use is 5349797990 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:19 [core.py:218] init engine (profile, create kv cache, warmup model) took 23.12 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:20 [factory.py:51] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: b97971e1-1552-4813-96ec-37228d14baad\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m WARNING 09-28 16:02:20 [base.py:84] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:02:20,712] LMCache WARNING:\u001b[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. \u001b[3m(utils.py:48:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:02:20,713] LMCache WARNING:\u001b[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE \u001b[3m(utils.py:52:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,714] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '5.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,715] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '16.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:02:20,716] LMCache WARNING:\u001b[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. \u001b[3m(utils.py:48:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[33;20m[2025-09-28 16:02:20,717] LMCache WARNING:\u001b[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE \u001b[3m(utils.py:52:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,718] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '5.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,719] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '16.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,720] LMCache INFO:\u001b[0m lmcache lookup client connect to tp_rank 0 with socket path ipc:///tmp/engine_b97971e1-1552-4813-96ec-37228d14baad_service_lookup_lmcache_rpc_port_0 \u001b[3m(lmcache_lookup_client.py:62:lmcache.v1.lookup_client.lmcache_lookup_client)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,722] LMCache INFO:\u001b[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=0, port=6999, socket_path=None, include_index_list=None \u001b[3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:20,723] LMCache INFO:\u001b[0m LMCache initialized for role KVConnectorRole.SCHEDULER with version 0.3.6-gcad4db105, vllm version 0.10.2, lmcache cache_engine metadata: None \u001b[3m(vllm_v1_adapter.py:668:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m INFO 09-28 16:02:20 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
      "INFO 09-28 16:02:21 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 09-28 16:02:21 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "vLLM Engine initialized successfully.\n",
      "\n",
      "--- (A) RECOMPUTE (Cache Miss: Full Prefill) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ce6e126ba74f808820e7e85d997353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:21,033] LMCache INFO:\u001b[0m Reqid: 0, Total tokens 824, LMCache hit tokens: 0, need to load: 0 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m WARNING 09-28 16:02:21 [cudagraph_dispatcher.py:102] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:21,038] LMCache INFO:\u001b[0m Post-initializing LMCacheEngine \u001b[3m(cache_engine.py:191:lmcache.v1.cache_engine)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5e677922af4d5bab345b6dc5081ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,449] LMCache INFO:\u001b[0m Storing KV cache for 824 out of 824 tokens (skip_leading_tokens=0) for request 0 \u001b[3m(vllm_v1_adapter.py:988:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,469] LMCache INFO:\u001b[0m Stored 824 out of total 824 tokens. size: 0.1509 gb, cost 13.4427 ms, throughput: 11.2238 GB/s; offload_time: 13.4179 ms, put_time: 0.0248 ms \u001b[3m(cache_engine.py:309:lmcache.v1.cache_engine)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,479] LMCache INFO:\u001b[0m Reqid: 1, Total tokens 824, LMCache hit tokens: 768, need to load: -32 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,480] LMCache INFO:\u001b[0m Reqid: 2, Total tokens 824, LMCache hit tokens: 768, need to load: -32 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,482] LMCache INFO:\u001b[0m Reqid: 3, Total tokens 824, LMCache hit tokens: 768, need to load: -32 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,483] LMCache INFO:\u001b[0m Reqid: 4, Total tokens 824, LMCache hit tokens: 768, need to load: -32 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:02:24,484] LMCache INFO:\u001b[0m Reqid: 5, Total tokens 824, LMCache hit tokens: 768, need to load: -32 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41.66s | Tokens: 5520 | Peak VRAM: 0.00 GB\n",
      "\n",
      "--- (B) WARM-UP (Cache Creation) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee5703a078e42e1b791eb1e4dfc64cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:02,691] LMCache INFO:\u001b[0m Reqid: 6, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab044f4a604096b7c125b058f8efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:03,194] LMCache INFO:\u001b[0m Reqid: 7, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:03,195] LMCache INFO:\u001b[0m Reqid: 8, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:03,197] LMCache INFO:\u001b[0m Reqid: 9, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:03,198] LMCache INFO:\u001b[0m Reqid: 10, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:03,199] LMCache INFO:\u001b[0m Reqid: 11, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29.61s | Tokens: 5490 | Peak VRAM: 0.00 GB\n",
      "\n",
      "--- (B) REUSE (Prefix Cache Hit & KV Reload) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cd3a5f99a54207818cdf5dbacdc718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,298] LMCache INFO:\u001b[0m Reqid: 12, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3819c91a2f17485f9481eab2a94e90f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,648] LMCache INFO:\u001b[0m Reqid: 13, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,649] LMCache INFO:\u001b[0m Reqid: 14, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,651] LMCache INFO:\u001b[0m Reqid: 15, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,652] LMCache INFO:\u001b[0m Reqid: 16, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=187370)\u001b[0;0m \u001b[32;20m[2025-09-28 16:03:32,653] LMCache INFO:\u001b[0m Reqid: 17, Total tokens 819, LMCache hit tokens: 768, need to load: -48 \u001b[3m(vllm_v1_adapter.py:1104:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29.15s | Tokens: 5490 | Peak VRAM: 0.00 GB\n",
      "\n",
      "=== Throughput (tokens/sec) Comparison ===\n",
      "1. Recompute TPS (Baseline): 132.51 t/s\n",
      "2. Warm-up TPS (First Run):  185.41 t/s\n",
      "3. Reuse TPS (Cached/Offloaded): 188.36 t/s\n",
      "\n",
      "Expected Result: Reuse TPS (188.36) should be significantly higher than Recompute TPS (132.51), confirming the benefit of prefix caching and tiered memory reuse.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "!pip install lmcache\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.config import KVTransferConfig \n",
    "# Environment variables for LMCache:\n",
    "os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n",
    "# ...\n",
    "# vLLM code explicitly calling the LMCache connector:\n",
    "kv_cfg = KVTransferConfig(kv_connector=\"LMCacheConnectorV1\", kv_role=\"kv_both\")\n",
    "\n",
    "# --- CONFIGURATION CONSTANTS ---\n",
    "MODEL = \"Qwen/Qwen1.5-MoE-A2.7B\" \n",
    "OFFLOAD_GB = 16  # Safer size for KV cache offload pool on CPU/CXL RAM\n",
    "SWAP_GB = 4      # Reduced swap space for PagedAttention block metadata\n",
    "MARGIN_GB = 0.50 # Safety margin for GPU VRAM\n",
    "SAFE_UTIL_LIMIT = 0.70 # Max fraction of free VRAM to use for model + hot cache\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "\n",
    "def query_all_gpus():\n",
    "    \"\"\"Queries NVIDIA SMI for GPU index, total memory, and free memory in GiB.\"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=index,memory.total,memory.free\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ]).decode().strip().splitlines()\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"[WARNING] nvidia-smi failed. Assuming a single GPU with 16GB total, 8GB free.\")\n",
    "        # Fallback for environments without nvidia-smi\n",
    "        return [(0, 16.0, 8.0)] \n",
    "    \n",
    "    gpus = []\n",
    "    for line in out:\n",
    "        idx_str, tot_mb, free_mb = [s.strip() for s in line.split(\",\")]\n",
    "        # Convert MB to GB\n",
    "        gpus.append((int(idx_str), int(tot_mb) / 1024.0, int(free_mb) / 1024.0)) \n",
    "    return gpus\n",
    "\n",
    "def run_batch(llm: LLM, prompts: list[str], sp: SamplingParams):\n",
    "    \"\"\"Generates text, calculates total tokens, time, and peak VRAM usage.\"\"\"\n",
    "    # Reset stats to measure peak VRAM usage only during this batch\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    t0 = time.time()\n",
    "    outs = llm.generate(prompts, sp)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # Calculate total processed tokens (Prefill + Decode)\n",
    "    total_tokens = 0\n",
    "    for out in outs:\n",
    "        # Prompt length (prefill tokens) + generated length (decode tokens)\n",
    "        total_tokens += len(out.prompt_token_ids)\n",
    "        total_tokens += sum(len(o.token_ids) for o in out.outputs)\n",
    "\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "    return (t1 - t0), total_tokens, peak, outs\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1) Pick emptiest GPU and set environment\n",
    "    gpus = query_all_gpus()\n",
    "    best = max(gpus, key=lambda x: x[2])\n",
    "    best_idx, total_gb, free_gb = best\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(best_idx)\n",
    "    print(f\"[GPU] Using GPU {best_idx} (free {free_gb:.2f} / total {total_gb:.2f} GiB)\")\n",
    "\n",
    "    if free_gb <= MARGIN_GB:\n",
    "        raise RuntimeError(\"Not enough free VRAM for safety margin. Please free up GPU resources.\")\n",
    "\n",
    "    # Calculate safe VRAM utilization limit\n",
    "    calculated_util = (free_gb - MARGIN_GB) / total_gb \n",
    "    safe_util = max(0.05, min(calculated_util * 0.90, SAFE_UTIL_LIMIT))\n",
    "    print(f\"[GPU] gpu_memory_utilization set to {safe_util:.3f} (conservative limit)\")\n",
    "\n",
    "    # 2) Configure LMCache and Pytorch for Tiered Offloading\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"VLLM_WORKER_MULTIPROCESSING\"] = \"0\"\n",
    "    \n",
    "    # LMCache environment variables for CPU/CXL offloading\n",
    "    os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n",
    "    os.environ[\"LMCACHE_MAX_LOCAL_CPU_SIZE\"] = str(float(OFFLOAD_GB))\n",
    "    os.environ[\"LMCACHE_CHUNK_SIZE\"] = \"256\"\n",
    "    print(f\"[CPU/CXL] Configuring LMCache for {OFFLOAD_GB} GB offload size...\")\n",
    "\n",
    "    # 3) Build vLLM Engine\n",
    "    print(f\"Building engine for {MODEL} (CPU/CXL offload + Prefix Caching)â€¦\")\n",
    "    \n",
    "    # KVTransferConfig is needed to explicitly enable the LMCache backend connector\n",
    "    kv_cfg = KVTransferConfig(kv_connector=\"LMCacheConnectorV1\", kv_role=\"kv_both\")\n",
    "    \n",
    "    torch.cuda.empty_cache(); gc.collect()\n",
    "    \n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=MODEL,\n",
    "            dtype=\"half\",\n",
    "            max_model_len=8192,\n",
    "            gpu_memory_utilization=safe_util,\n",
    "            cpu_offload_gb=OFFLOAD_GB,       # ðŸŒŸ KV Cache Offloading Tier\n",
    "            enable_prefix_caching=True,     # ðŸŒŸ Prefix Caching Optimization\n",
    "            kv_transfer_config=kv_cfg,\n",
    "            enforce_eager=True,\n",
    "            swap_space=SWAP_GB,             # PagedAttention block swapping\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n[ERROR] Failed to initialize vLLM Engine. Check memory configuration.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # Hint to the user about common issues:\n",
    "        print(f\"HINT: Try reducing OFFLOAD_GB ({OFFLOAD_GB}) and SWAP_GB ({SWAP_GB}) to lower CPU/CXL memory pressure.\")\n",
    "        exit(1)\n",
    "\n",
    "    print(\"vLLM Engine initialized successfully.\")\n",
    "    \n",
    "    # 4) Run Experiments to Demonstrate Benefits\n",
    "    sp = SamplingParams(temperature=0.0, max_tokens=96)\n",
    "    \n",
    "    # Create a long prefix (e.g., 2000 tokens) to stress the cache\n",
    "    base_prefix = \"System: You are a precise and helpful assistant. \" + (\"Data chunk \" * 400) \n",
    "\n",
    "    # --- (A) RECOMPUTE: Cache Miss Simulation ---\n",
    "    # The changing suffix/salt prevents a prefix cache hit, forcing full prefill computation.\n",
    "    recompute_prompts = [\n",
    "        f\"{base_prefix}\\nUser: Q{idx} summarize the data. [session={idx%3}]\"\n",
    "        for idx in range(6)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- (A) RECOMPUTE (Cache Miss: Full Prefill) ---\")\n",
    "    dt_r, toks_r, vram_r, _ = run_batch(llm, recompute_prompts, sp)\n",
    "    print(f\"Time: {dt_r:.2f}s | Tokens: {toks_r} | Peak VRAM: {vram_r:.2f} GB\")\n",
    "\n",
    "    # --- (B) NO-RECOMPUTE: Cache Hit Simulation ---\n",
    "    same_prompts = [\n",
    "        f\"{base_prefix}\\nUser: Q{idx} summarize the data.\" # Exact same prefix\n",
    "        for idx in range(6)\n",
    "    ]\n",
    "    \n",
    "    # Run 1 (Warm-up): Computes prefix, stores/offloads KV cache.\n",
    "    print(\"\\n--- (B) WARM-UP (Cache Creation) ---\")\n",
    "    dt_w, toks_w, vram_w, _ = run_batch(llm, same_prompts, sp)\n",
    "    print(f\"Time: {dt_w:.2f}s | Tokens: {toks_w} | Peak VRAM: {vram_w:.2f} GB\")\n",
    "    \n",
    "    # Run 2 (Reuse): Reuses the cached KV blocks from the CPU/CXL tier.\n",
    "    print(\"\\n--- (B) REUSE (Prefix Cache Hit & KV Reload) ---\")\n",
    "    dt_n, toks_n, vram_n, _ = run_batch(llm, same_prompts, sp) \n",
    "    print(f\"Time: {dt_n:.2f}s | Tokens: {toks_n} | Peak VRAM: {vram_n:.2f} GB\")\n",
    "\n",
    "    # --- Results Summary ---\n",
    "    print(\"\\n=== Throughput (tokens/sec) Comparison ===\")\n",
    "    tps_r = toks_r/max(dt_r,1e-6)\n",
    "    tps_w = toks_w/max(dt_w,1e-6)\n",
    "    tps_n = toks_n/max(dt_n,1e-6)\n",
    "    \n",
    "    print(f\"1. Recompute TPS (Baseline): {tps_r:.2f} t/s\")\n",
    "    print(f\"2. Warm-up TPS (First Run):  {tps_w:.2f} t/s\")\n",
    "    print(f\"3. Reuse TPS (Cached/Offloaded): {tps_n:.2f} t/s\")\n",
    "    \n",
    "    print(f\"\\nExpected Result: Reuse TPS ({tps_n:.2f}) should be significantly higher than Recompute TPS ({tps_r:.2f}), confirming the benefit of prefix caching and tiered memory reuse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7f3f89-741a-495c-acbf-e000bbce58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] Using GPU 0 | Total VRAM: 23.99 GiB\n",
      "[CPU/CXL] Offload configured for 16 GB.\n",
      "INFO 09-28 17:19:46 [utils.py:328] non-default args: {'dtype': 'half', 'max_model_len': 8192, 'enable_prefix_caching': True, 'cpu_offload_gb': 16, 'gpu_memory_utilization': 0.23313385442110404, 'disable_log_stats': True, 'enforce_eager': True, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='882f0285-b90a-4b87-b1a2-e0b6937049a7', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None), 'model': 'Qwen/Qwen1.5-MoE-A2.7B'}\n",
      "INFO 09-28 17:19:47 [__init__.py:742] Resolved architecture: Qwen2MoeForCausalLM\n",
      "WARNING 09-28 17:19:47 [__init__.py:2767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-28 17:19:47 [__init__.py:1815] Using max model len 8192\n",
      "INFO 09-28 17:19:47 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-28 17:19:47 [__init__.py:3400] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:19:49 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:19:49 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen1.5-MoE-A2.7B', speculative_config=None, tokenizer='Qwen/Qwen1.5-MoE-A2.7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen1.5-MoE-A2.7B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:19:50 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:19:50 [factory.py:51] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 882f0285-b90a-4b87-b1a2-e0b6937049a7\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m WARNING 09-28 17:19:50 [base.py:84] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[33;20m[2025-09-28 17:19:50,996] LMCache WARNING:\u001b[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. \u001b[3m(utils.py:48:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[33;20m[2025-09-28 17:19:50,997] LMCache WARNING:\u001b[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE \u001b[3m(utils.py:52:lmcache.integration.vllm.utils)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:50,998] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '5.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:50,999] LMCache INFO:\u001b[0m LMCache Configuration: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': '16.0 GB', 'local_disk': None, 'max_local_disk_size': '0.0 GB', 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(config.py:444:lmcache.v1.config)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:51,000] LMCache INFO:\u001b[0m use mla: False, kv shape: (24, 2, 256, 16, 128), num_mtp_layers:0 \u001b[3m(vllm_v1_adapter.py:460:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:51,005] LMCache INFO:\u001b[0m Creating LMCacheEngine instance vllm-instance \u001b[3m(cache_engine.py:1447:lmcache.v1.cache_engine)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:51,006] LMCache INFO:\u001b[0m NUMA mapping for instance vllm-instance: None \u001b[3m(cache_engine.py:1450:lmcache.v1.cache_engine)\u001b[0m\n",
      "[W928 17:19:50.002395143 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:59,653] LMCache INFO:\u001b[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 16.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'lookup_url': None, 'distributed_url': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_default_instance', 'controller_url': None, 'lmcache_worker_port': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_nixl': False, 'nixl_role': None, 'nixl_receiver_host': None, 'nixl_receiver_port': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'nixl_enable_gc': False, 'nixl_backends': None, 'enable_xpyd': False, 'nixl_peer_host': None, 'nixl_peer_init_port': None, 'nixl_peer_alloc_port': None, 'nixl_proxy_host': None, 'nixl_proxy_port': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None} \u001b[3m(cache_engine.py:95:lmcache.v1.cache_engine)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:59,655] LMCache INFO:\u001b[0m Initializing LRUCachePolicy \u001b[3m(lru.py:20:lmcache.v1.storage_backend.cache_policy.lru)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:19:59,656] LMCache INFO:\u001b[0m Initializing usage context. \u001b[3m(usage_context.py:261:lmcache.usage_context)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:20:04,213] LMCache INFO:\u001b[0m lmcache lookup server start on ipc:///tmp/engine_882f0285-b90a-4b87-b1a2-e0b6937049a7_service_lookup_lmcache_rpc_port_0 \u001b[3m(lmcache_lookup_client.py:215:lmcache.v1.lookup_client.lmcache_lookup_client)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:20:04,215] LMCache INFO:\u001b[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None \u001b[3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)\u001b[0m\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m \u001b[32;20m[2025-09-28 17:20:04,216] LMCache INFO:\u001b[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.6-gcad4db105, vllm version 0.10.2, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen1.5-MoE-A2.7B', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.float16, kv_shape=(24, 2, 256, 16, 128), use_mla=False) \u001b[3m(vllm_v1_adapter.py:668:lmcache.integration.vllm.vllm_v1_adapter)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m WARNING 09-28 17:20:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:20:04 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen1.5-MoE-A2.7B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:20:04 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m INFO 09-28 17:20:04 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 45, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     model = initialize_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py\", line 64, in initialize_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 518, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.model = Qwen2MoeModel(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 199, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 352, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 642, in make_layers\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 643, in <listcomp>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 354, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     lambda prefix: Qwen2MoeDecoderLayer(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 295, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.mlp = Qwen2MoeSparseMoeBlock(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 114, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.experts = FusedMoE(num_experts=config.num_experts,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 945, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     self.quant_method.create_weights(layer=self, **moe_quant_params)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 313, in create_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     w2_weight = torch.nn.Parameter(torch.empty(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 103, in __torch_function__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m ERROR 09-28 17:20:23 [core.py:718] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 330.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 340.00 MiB is free. Process 1008387 has 16.87 GiB memory in use. Process 1012929 has 6.30 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 6.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 722, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 709, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 505, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core.py\", line 82, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/utils/__init__.py\", line 3060, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2371, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 45, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     model = initialize_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/utils.py\", line 64, in initialize_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 518, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.model = Qwen2MoeModel(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/compilation/decorators.py\", line 199, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 352, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 642, in make_layers\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 643, in <listcomp>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 354, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     lambda prefix: Qwen2MoeDecoderLayer(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 295, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.mlp = Qwen2MoeSparseMoeBlock(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2_moe.py\", line 114, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.experts = FusedMoE(num_experts=config.num_experts,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 945, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 313, in create_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     w2_weight = torch.nn.Parameter(torch.empty(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 103, in __torch_function__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8708)\u001b[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 330.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 340.00 MiB is free. Process 1008387 has 16.87 GiB memory in use. Process 1012929 has 6.30 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 6.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m recompute_prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser: Q\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m summarize the data. [session=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m)]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Engine A: Full optimization (Prefix Caching + CXL Offload)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m llm_optimized \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_util\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOFFLOAD_GB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSWAP_GB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Engine B: Baseline (Standard PagedAttention, NO Prefix Caching/Offload)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m llm_base \u001b[38;5;241m=\u001b[39m build_llm(MODEL, safe_util, \u001b[38;5;241m0\u001b[39m, SWAP_GB, enable_prefix_caching\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m, in \u001b[0;36mbuild_llm\u001b[0;34m(model_name, util_limit, offload_gb, swap_gb, enable_prefix_caching)\u001b[0m\n\u001b[1;32m     58\u001b[0m kv_cfg \u001b[38;5;241m=\u001b[39m KVTransferConfig(kv_connector\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLMCacheConnectorV1\u001b[39m\u001b[38;5;124m\"\u001b[39m, kv_role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkv_both\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutil_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_transfer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Only pass LMCache config if caching is on\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mswap_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py:282\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py:493\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    491\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/llm_engine.py:134\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/llm_engine.py:111\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/v1/engine/core_client.py:495\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    493\u001b[0m sync_input_socket \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mSocket\u001b[38;5;241m.\u001b[39mshadow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_socket)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m identities:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43msync_input_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600_000\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimed out waiting for engines to send\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial message on input socket.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    498\u001b[0m     identity, _ \u001b[38;5;241m=\u001b[39m sync_input_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py:1062\u001b[0m, in \u001b[0;36mSocket.poll\u001b[0;34m(self, timeout, flags)\u001b[0m\n\u001b[1;32m   1060\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poller_class()\n\u001b[1;32m   1061\u001b[0m p\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m, flags)\n\u001b[0;32m-> 1062\u001b[0m evts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# return 0 if no events, otherwise return event bitfield\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/poll.py:106\u001b[0m, in \u001b[0;36mPoller.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    105\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mzmq_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msockets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/_zmq.py:1680\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.zmq_poll\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/_zmq.py:179\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.config import KVTransferConfig\n",
    "\n",
    "# --- CONFIGURATION CONSTANTS ---\n",
    "MODEL = \"Qwen/Qwen1.5-MoE-A2.7B\" \n",
    "OFFLOAD_GB = 16  # Safer size for KV cache offload pool on CPU/CXL RAM\n",
    "SWAP_GB = 4      # Reduced swap space for PagedAttention block metadata\n",
    "MARGIN_GB = 0.50 # Safety margin for GPU VRAM\n",
    "SAFE_UTIL_LIMIT = 0.70 # Max fraction of free VRAM to use for model + hot cache\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "\n",
    "def query_all_gpus():\n",
    "    \"\"\"Queries NVIDIA SMI for GPU index, total memory, and free memory in GiB.\"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            \"nvidia-smi\",\n",
    "            \"--query-gpu=index,memory.total,memory.free\",\n",
    "            \"--format=csv,noheader,nounits\"\n",
    "        ]).decode().strip().splitlines()\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"[WARNING] nvidia-smi failed. Assuming a single GPU with 16GB total, 8GB free.\")\n",
    "        return [(0, 16.0, 8.0)]  # Fallback\n",
    "    \n",
    "    gpus = []\n",
    "    for line in out:\n",
    "        idx_str, tot_mb, free_mb = [s.strip() for s in line.split(\",\")]\n",
    "        gpus.append((int(idx_str), int(tot_mb) / 1024.0, int(free_mb) / 1024.0)) \n",
    "    return gpus\n",
    "\n",
    "def run_batch(llm: LLM, prompts: list[str], sp: SamplingParams):\n",
    "    \"\"\"Generates text and calculates total tokens and time.\"\"\"\n",
    "    t0 = time.time()\n",
    "    outs = llm.generate(prompts, sp)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    for out in outs:\n",
    "        # Prompt length (prefill tokens) + generated length (decode tokens)\n",
    "        total_tokens += len(out.prompt_token_ids)\n",
    "        total_tokens += sum(len(o.token_ids) for o in out.outputs)\n",
    "\n",
    "    # Note: Peak VRAM measurement is removed here, as the meaningful metric is\n",
    "    # the static allocated VRAM measured after engine creation (see main block).\n",
    "    return (t1 - t0), total_tokens, outs\n",
    "\n",
    "def get_current_vram_usage():\n",
    "    \"\"\"Fix: Measure the total allocated VRAM block after engine init.\"\"\"\n",
    "    return torch.cuda.memory_allocated() / (1024**3)\n",
    "\n",
    "def build_llm(model_name, util_limit, offload_gb, swap_gb, enable_prefix_caching):\n",
    "    \"\"\"Factory function to build vLLM engine with specific caching config.\"\"\"\n",
    "    kv_cfg = KVTransferConfig(kv_connector=\"LMCacheConnectorV1\", kv_role=\"kv_both\")\n",
    "    \n",
    "    try:\n",
    "        llm = LLM(\n",
    "            model=model_name,\n",
    "            dtype=\"half\",\n",
    "            max_model_len=8192,\n",
    "            gpu_memory_utilization=util_limit,\n",
    "            cpu_offload_gb=offload_gb,\n",
    "            enable_prefix_caching=enable_prefix_caching,\n",
    "            kv_transfer_config=kv_cfg if enable_prefix_caching else None, # Only pass LMCache config if caching is on\n",
    "            enforce_eager=True,\n",
    "            swap_space=swap_gb,\n",
    "        )\n",
    "        return llm\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\n[ERROR] Failed to initialize vLLM Engine. Config: prefix_caching={enable_prefix_caching}\")\n",
    "        print(f\"Root Error: {e}\")\n",
    "        # Return None instead of calling exit(1) to allow interactive environment to continue\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1) Setup GPU Environment ---\n",
    "    gpus = query_all_gpus()\n",
    "    best = max(gpus, key=lambda x: x[2])\n",
    "    best_idx, total_gb, free_gb = best\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(best_idx)\n",
    "\n",
    "    calculated_util = (free_gb - MARGIN_GB) / total_gb \n",
    "    safe_util = max(0.05, min(calculated_util * 0.90, SAFE_UTIL_LIMIT))\n",
    "\n",
    "    # --- 2) Configure LMCache Environment Variables ---\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"VLLM_WORKER_MULTIPROCESSING\"] = \"0\"\n",
    "    os.environ[\"LMCACHE_LOCAL_CPU\"] = \"True\"\n",
    "    os.environ[\"LMCACHE_MAX_LOCAL_CPU_SIZE\"] = str(float(OFFLOAD_GB))\n",
    "    os.environ[\"LMCACHE_CHUNK_SIZE\"] = \"256\"\n",
    "    \n",
    "    print(f\"[GPU] Using GPU {best_idx} | Total VRAM: {total_gb:.2f} GiB\")\n",
    "    print(f\"[CPU/CXL] Offload configured for {OFFLOAD_GB} GB.\")\n",
    "\n",
    "    # --- 3) Build Engines ---\n",
    "    sp = SamplingParams(temperature=0.0, max_tokens=96)\n",
    "    base_prefix = \"System: You are a precise and helpful assistant. \" + (\"Data chunk \" * 400)\n",
    "    same_prompts = [f\"{base_prefix}\\nUser: Q{idx} summarize the data.\" for idx in range(6)]\n",
    "    recompute_prompts = [f\"{base_prefix}\\nUser: Q{idx} summarize the data. [session={idx%3}]\" for idx in range(6)]\n",
    "\n",
    "    # Engine A: Full optimization (Prefix Caching + CXL Offload)\n",
    "    llm_optimized = build_llm(MODEL, safe_util, OFFLOAD_GB, SWAP_GB, enable_prefix_caching=True)\n",
    "    \n",
    "    # Engine B: Baseline (Standard PagedAttention, NO Prefix Caching/Offload)\n",
    "    llm_base = build_llm(MODEL, safe_util, 0, SWAP_GB, enable_prefix_caching=False)\n",
    "\n",
    "    # --- 4) Conditional Execution ---\n",
    "    if llm_optimized is None or llm_base is None:\n",
    "        print(\"\\n[FATAL] Skipping performance test because one or both engines failed to initialize.\")\n",
    "        # If the engine is None, the VRAM usage will be 0.00 GB, which is expected.\n",
    "    else:\n",
    "        # Only proceed if both engines initialized successfully\n",
    "        \n",
    "        vram_opt = get_current_vram_usage()\n",
    "        print(f\"\\n[VRAM Optimized Engine] Total allocated VRAM (Weights + Cache Pool): {vram_opt:.2f} GB\")\n",
    "        \n",
    "        vram_base = get_current_vram_usage()\n",
    "        print(f\"[VRAM Baseline Engine] Total allocated VRAM (Weights + Cache Pool): {vram_base:.2f} GB\")\n",
    "        \n",
    "        # BASELINE 1: NO Prefix Caching, Full Computation\n",
    "        print(\"\\n--- BASELINE (C) NO PREFIX CACHING ---\")\n",
    "        dt_b, toks_b, _ = run_batch(llm_base, same_prompts, sp)\n",
    "        print(f\"Time: {dt_b:.2f}s | Tokens: {toks_b}\")\n",
    "\n",
    "\n",
    "        # BASELINE 2: Optimized Engine, Forced Cache Miss (Recompute)\n",
    "        print(\"\\n--- BASELINE (A) FULL RECOMPUTATION (Optimized Engine) ---\")\n",
    "        dt_r, toks_r, _ = run_batch(llm_optimized, recompute_prompts, sp)\n",
    "        print(f\"Time: {dt_r:.2f}s | Tokens: {toks_r}\")\n",
    "\n",
    "\n",
    "        # OPTIMIZED RUN: Prefix Cache Hit (Reuse from CXL)\n",
    "        print(\"\\n--- OPTIMIZED (B) WARM-UP (Cache Creation & Offload) ---\")\n",
    "        dt_w, toks_w, _ = run_batch(llm_optimized, same_prompts, sp)\n",
    "        print(f\"Time: {dt_w:.2f}s | Tokens: {toks_w}\")\n",
    "\n",
    "        print(\"\\n--- OPTIMIZED (B) REUSE (Cache Hit & CXL Reload) ---\")\n",
    "        dt_n, toks_n, _ = run_batch(llm_optimized, same_prompts, sp) \n",
    "        print(f\"Time: {dt_n:.2f}s | Tokens: {toks_n}\")\n",
    "\n",
    "\n",
    "        # --- 5) Results Summary ---\n",
    "        print(\"\\n=== Throughput (tokens/sec) Comparison ===\")\n",
    "        tps_b = toks_b/max(dt_b,1e-6)\n",
    "        tps_r = toks_r/max(dt_r,1e-6)\n",
    "        tps_w = toks_w/max(dt_w,1e-6)\n",
    "        tps_n = toks_n/max(dt_n,1e-6)\n",
    "        \n",
    "        print(f\"1. No Prefix Cache (Baseline): {tps_b:.2f} t/s\")\n",
    "        print(f\"2. Cache Miss (Recompute):     {tps_r:.2f} t/s\")\n",
    "        print(f\"3. Warm-up Run (First Cache):  {tps_w:.2f} t/s\")\n",
    "        print(f\"4. Reuse Run (CXL Hit):        {tps_n:.2f} t/s\")\n",
    "        \n",
    "        print(\"\\nConclusion: The significant difference between Line 1/2 and Line 4 demonstrates that the **Prefix Caching** feature, backed by **CXL Offloading**, successfully skips the expensive prefill step and reloads the required data quickly, boosting throughput.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3077898-ec01-4a2d-88a0-e4a229266962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
